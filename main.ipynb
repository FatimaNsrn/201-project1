{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae6431f",
   "metadata": {},
   "source": [
    "importing necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25390194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"ajaykarthick/imdb-movie-reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991ca2d",
   "metadata": {},
   "source": [
    "Info on the dataset\n",
    "Display how many 1 and 0 in label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ab9244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['review', 'label'],\n",
      "        num_rows: 40000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['review', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "Number of 0s: 20000\n",
      "Number of 1s: 20000\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "\n",
    "#use the command to tell me how many 0 and 1 are in the label column of the training set\n",
    "train_labels = ds['train']['label']\n",
    "num_zeros = train_labels.count(0)\n",
    "num_ones = train_labels.count(1)\n",
    "print(f\"Number of 0s: {num_zeros}\")\n",
    "print(f\"Number of 1s: {num_ones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fadffb",
   "metadata": {},
   "source": [
    "preprocessing for Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e72bc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Removes common HTML break tags from the 'review' column.\"\"\"\n",
    "    \n",
    "    # Use re.sub to replace the pattern with an empty string\n",
    "    # The pattern r'<br\\s*/?>' matches <br>, <br/>, or <br /> (case-insensitive)\n",
    "    CLEANR = re.compile(r'<br\\s*/?>', re.IGNORECASE)\n",
    "    \n",
    "    text[\"review\"] = re.sub(CLEANR, ' ', text[\"review\"])\n",
    "    \n",
    "    return text\n",
    "\n",
    "clear_output = ds['train'].map(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5751a11c",
   "metadata": {},
   "source": [
    "Printing the first row to check if preprocessing working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb405f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': \"Ms Aparna Sen, the maker of Mr & Mrs Iyer, directs this movie about a young girl's struggle to cope with her debilitating condition.  Meethi (Konkona Sen) has been an aloof kid ever since childhood and has shown signs of delusion, no one knows why. The dormant tendency however slips out of control, when the job assignment takes her to neighboring Bihar where she's raped by some political goons. The resulting trauma also leads to episodes of manic-depressive psychosis in addition to her schizophrenia. She careens out of control over the years, progressively getting worse and sinking deeper into her private 'world'.  The juxtaposition of an 'unsettled' (divorced) elder sister and how her domineering ways make an already bad situation worse, is indicative of what a fine line there is between abnormal and *seemingly normal*. Ms Sen also makes an excellent commentary on the social alienation of such individuals. Social rehab is standard therapy along with all the deadly mind-altering drugs. But what about the poor and the destitute, who're always left to fend for themselves and usually fall by the wayside?  The romantic connection between Dr Kunal and Anu was unnecessary. Also the cafeteria scene where Dr Kunal explains to Anu how real their world really is to them, was redundant. Anu should already know all that. The English dialog is a bit awkward at times though the acting compensates for that. Konkona and Shabana prove that their reputation is every bit worth it. Waheeda, Rahul and Shefali play their limited roles very well.   Extensive research seems to have been done about this illness, its very evident. But its not clear if MDP can coexist with schizophrenia in the same patient, side-by-side. Also in the early part, Dr Kunal recommends E.C.T (shock therapy) while invalidating the fact that it doesn't work for schizophrenics, only for extreme MDP with suicidal tendencies and other forms of bipolar disorder.  The ending of the remarkable story is suggestive of an unknown solution (maybe no solution). The movie could have ended on a nicer note, since worldwide the mentally ill can and do lead balanced and fruitful if not very fulfilling, lives under good medical care.  Nonetheless, its an excellent film made with extreme sensitivity to the subject. HATS OFF to Ms Sen! No one in India could've done it better.\", 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "#Display the first example from the cleaned training set\n",
    "print(clear_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f9a23",
   "metadata": {},
   "source": [
    "Remove punctuations lower case everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e1ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_text_cleaning(Review):\n",
    "    \"\"\"\n",
    "    Converts text to lowercase and removes punctuation/special characters\n",
    "    for the 'review' column.\n",
    "    \"\"\"\n",
    "    text = Review[\"review\"]\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove Punctuation & Special Characters\n",
    "    # The pattern r'[^a-z0-9\\s]' matches anything that is NOT a letter, number, or space (not gonna lie this came from AI ,I did it the long way but Ai told me there is a faster and more efficient way to do it so I used it)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    \n",
    "    # Update the 'review' column in the Review dictionary\n",
    "    Review[\"review\"] = text\n",
    "    \n",
    "    return Review\n",
    "\n",
    "cleaned_dataset = clear_output.map(simple_text_cleaning)\n",
    "#Display the first example to test it but commented out to reduce output\n",
    "#print(cleaned_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06d8f5",
   "metadata": {},
   "source": [
    "now Time for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7098c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF train shape: (40000, 20000)\n",
      "TF-IDF test shape : (10000, 20000)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vectorization with unigrams + bigrams for Naive Bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Use the cleaned texts produced earlier (you've already removed HTML, punctuation, and lowercased)\n",
    "train_texts = [ex['review'] for ex in cleaned_dataset]\n",
    "y_train = [ex['label'] for ex in cleaned_dataset]\n",
    "\n",
    "# Prepare test texts using the same preprocessing steps\n",
    "cleaned_test = ds['test'].map(remove_html_tags).map(simple_text_cleaning)\n",
    "test_texts = [ex['review'] for ex in cleaned_test]\n",
    "y_test = [ex['label'] for ex in cleaned_test]\n",
    "\n",
    "# Create TF-IDF vectorizer with ngrams (1,2). Tune max_features as needed.\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,2), max_features=20000)\n",
    "X_train_tfidf = tfidf_vect.fit_transform(train_texts)\n",
    "X_test_tfidf = tfidf_vect.transform(test_texts)\n",
    "\n",
    "print('TF-IDF train shape:', X_train_tfidf.shape)\n",
    "print('TF-IDF test shape :', X_test_tfidf.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc210385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      5000\n",
      "           1       0.88      0.86      0.87      5000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[4421  579]\n",
      " [ 709 4291]]\n",
      "Naive Bayes Test accuracy: 0.8712\n",
      "Naive Bayes Train accuracy: 0.8919\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Train MultinomialNB classifier\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions and metrics on test set\n",
    "y_pred = nb.predict(X_test_tfidf)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('\\nClassification report (test):')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('\\nConfusion matrix (test):')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#training set performance\n",
    "y_pred_train = nb.predict(X_train_tfidf)\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "print(f'Naive Bayes Test accuracy: {test_acc:.4f}')\n",
    "print(f'Naive Bayes Train accuracy: {train_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089deff9",
   "metadata": {},
   "source": [
    "Now we chose use Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c98ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix shape: (40000, 20000)\n",
      "Test matrix shape: (10000, 20000)\n",
      "Test accuracy: 0.8909\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      5000\n",
      "           1       0.89      0.89      0.89      5000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[4475  525]\n",
      " [ 566 4434]]\n",
      "Train accuracy: 0.9997\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Prepare training data from cleaned_dataset \n",
    "train_texts = [ex['review'] for ex in cleaned_dataset]\n",
    "y_train = [ex['label'] for ex in cleaned_dataset]\n",
    "\n",
    "cleaned_test = ds['test'].map(remove_html_tags).map(simple_text_cleaning)\n",
    "test_texts = [ex['review'] for ex in cleaned_test]\n",
    "y_test = [ex['label'] for ex in cleaned_test]\n",
    "\n",
    "# Vectorize using Bag-of-Words (CountVectorizer) with unigrams + bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), max_features=20000)\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "\n",
    "print('Train matrix shape:', X_train.shape)\n",
    "print('Test matrix shape:', X_test.shape)\n",
    "\n",
    "# Train Logistic Regression (increase max_iter if convergence warnings appear)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "print('\\nClassification report (test):')\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('\\nConfusion matrix (test):')\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# Optional: evaluate on training set to check for overfitting\n",
    "y_pred_train = model.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "print(f'Train accuracy: {train_acc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec1246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8862\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88      5000\n",
      "           1       0.88      0.90      0.89      5000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[4359  641]\n",
      " [ 497 4503]]\n",
      "Train accuracy: 0.9674\n",
      "Test accuracy: 0.8862\n"
     ]
    }
   ],
   "source": [
    "# Train an LSTM classifier using Word2Vec embeddings as pretrained weights\n",
    "# This uses the Word2Vec model trained on the training tokens and a Keras LSTM for binary classification.\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Prepare texts and labels (use cleaned_dataset from earlier)\n",
    "train_texts = [ex['review'] for ex in cleaned_dataset]\n",
    "y_train = np.array([ex['label'] for ex in cleaned_dataset])\n",
    "\n",
    "# Clean and prepare test split similarly\n",
    "cleaned_test = ds['test'].map(remove_html_tags).map(simple_text_cleaning)\n",
    "test_texts = [ex['review'] for ex in cleaned_test]\n",
    "y_test = np.array([ex['label'] for ex in cleaned_test])\n",
    "\n",
    "# Tokenize texts (we'll use Keras Tokenizer to convert to sequences)\n",
    "max_words = 20000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "vocab_size = min(max_words, len(tokenizer.word_index)) + 1\n",
    "\n",
    "# Convert texts to padded sequences\n",
    "train_seqs = tokenizer.texts_to_sequences(train_texts)\n",
    "test_seqs = tokenizer.texts_to_sequences(test_texts)\n",
    "maxlen = 200\n",
    "X_train_seq = pad_sequences(train_seqs, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test_seq = pad_sequences(test_seqs, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# If a Word2Vec model isn't already trained in this session, train one on training tokens\n",
    "train_tokens = [txt.split() for txt in train_texts]\n",
    "w2v_params = dict(vector_size=100, window=5, min_count=2, workers=4, epochs=5)\n",
    "w2v = Word2Vec(sentences=train_tokens, **w2v_params)\n",
    "embed_dim = w2v.vector_size\n",
    "\n",
    "# Build embedding matrix aligned with tokenizer's word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    if word in w2v.wv.key_to_index:\n",
    "        embedding_matrix[i] = w2v.wv[word]\n",
    "    # otherwise row stays zeros (OOV)\n",
    "\n",
    "# Build LSTM model with pretrained embeddings (trainable=True to fine-tune)\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True),\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks: early stopping and best-model checkpoint\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('best_lstm_w2v.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model (small epochs by default; increase for better performance)\n",
    "history = model.fit(X_train_seq, y_train, epochs=6, batch_size=64, validation_split=0.1, callbacks=[es, mc])\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "model.load_weights('best_lstm_w2v.h5')\n",
    "y_pred_prob = model.predict(X_test_seq, batch_size=64).ravel()\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "print(f'Test accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print('\\nClassification report (test):')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('\\nConfusion matrix (test):')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "ddddd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
